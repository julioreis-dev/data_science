{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big Data: Como instalar o PySpark no Google Colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julioreis-dev/data_science/blob/main/Big_Data_Como_instalar_o_PySpark_no_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTruZS5TPEvK"
      },
      "source": [
        "# Big Data: Como instalar o PySpark no Google Colab\n",
        "\n",
        "Como instalar o PySpark no Google Colab √© uma d√∫vida comum entre aqueles que est√£o migrando seus projetos de Data Science para ambientes na nuvem.\n",
        "\n",
        "O termo Big Data est√° cada vez mais presente, e mesmo projetos pessoais podem assumir uma grande dimensionalidade devido √† quantidade de dados dispon√≠veis.\n",
        "\n",
        "Para analisar grandes volumes de dados, Big Data, com velocidade, o Apache Spark √© uma ferramenta muito utilizada, dada a sua capacidade de processamento de dados e computa√ß√£o paralela.\n",
        "\n",
        "O Spark foi pensado para ser acess√≠vel, oferecendo diversas APIs e frameworks em Python, Scala, SQL e diversas outras linguagens.\n",
        "\n",
        "## PySpark no Google Colab\n",
        "\n",
        "PySpark √© a interface alto n√≠vel que permite voc√™ conseguir acessar e usar o Spark por meio da linguagem Python. Usando o PySpark, voc√™ consegue escrever todo o seu c√≥digo usando apenas o nosso estilo Python de escrever c√≥digo.\n",
        "\n",
        "J√° o Google Colab √© uma ferramenta incr√≠vel, poderosa e gratuita ‚Äì com suporte de GPU inclusive. Uma vez que roda 100% na nuvem, voc√™ n√£o tem a necessidade de instalar qualquer coisa na sua pr√≥pria m√°quina.\n",
        "\n",
        "No entanto, apesar da maioria das bibliotecas de Data Science estarem previamente instaladas no Colab, o mesmo n√£o acontece com o PySpark. Para conseguir usar o PySpark √© necess√°rio alguns passos intermedi√°rios, que n√£o s√£o triviais para aqueles que est√£o come√ßando.\n",
        "\n",
        "Dessa maneira, preparei um tutorial simples e direto ensinando a instalar as depend√™ncias e a biblioteca.\n",
        "\n",
        "## Instalando o PySpark no Google Colab\n",
        "\n",
        "Instalar o PySpark n√£o √© um processo direto como de praxe em Python. N√£o basta usar um pip install apenas. Na verdade, antes de tudo √© necess√°rio instalar depend√™ncias como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz_8sWI7PKEl"
      },
      "source": [
        "# instalar as depend√™ncias\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabkOXVRPYgq"
      },
      "source": [
        "A pr√≥xima etapa √© configurar as vari√°veis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as depend√™ncias est√£o rodando.\n",
        "\n",
        "Para conseguir ‚Äúmanipular‚Äù o terminal e interagir como ele, voc√™ pode usar a biblioteca os."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpG11RQPbRf"
      },
      "source": [
        "# configurar as vari√°veis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"import√°vel\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klur5aiRPdqx"
      },
      "source": [
        "Com tudo pronto, vamos rodar uma sess√£o local para testar se a instala√ß√£o funcionou corretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpJqWggpPKXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef448b13-b787-4e24-b3f8-ea2332b39d11"
      },
      "source": [
        "# iniciar uma sess√£o local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# download do http para arquivo local\n",
        "# !wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2019-07-15/visualisations/listings.csv\n",
        "\n",
        "# carregar dados do Airbnb\n",
        "df_spark = sc.read.csv(\"/content/sample_data/california_housing_test.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# ver algumas informa√ß√µes sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5eswa9ZzWS"
      },
      "source": [
        "## Big Data e Python\n",
        "\n",
        "A biblioteca PySpark permite voc√™ criar seu servidor Apache Spark, trabalhar com grandes volumes de dados e at√© mesmo fazer streaming em tempo real.\n",
        "\n",
        "Na minha opini√£o, o Spark √© o melhor framework para trabalhar com Big Data. Tenha certeza que o PySpark vai te ajudar muito ao criar uma interface Python que permita a comunica√ß√£o entre seu projeto e o servidor.\n",
        "\n",
        "Neste artigo, o meu objetivo foi unicamente apresentar a biblioteca, al√©m de ensinar como voc√™ pode instal√°-la em um ambiente de nuvem gratuito, o Google Colab. Aproveite e comece a usar hoje mesmo üôÇ"
      ]
    }
  ]
}